{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Acquiring and Processing Information on the World's Largest Banks\n",
    "\n",
    "In this project, we will work with real-world data and perform the operations of Extraction, Transformation, and Loading (ETL) as required.\n",
    "\n",
    "- Task 1: Logging function\n",
    "- Task 2: Extraction of data\n",
    "- Task 3: Transformation of data\n",
    "- Task 4: Loading to CSV\n",
    "- Task 5: Loading to Database\n",
    "- Task 6: Function to Run queries on Database\n",
    "- Task 7: Verify log entries"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preliminaries: Installing libraries and downloading data\n",
    "\n",
    "Install the required libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: numpy in c:\\users\\haiho\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (2.2.6)\n",
      "Requirement already satisfied: pandas in c:\\users\\haiho\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (2.3.1)\n",
      "Requirement already satisfied: bs4 in c:\\users\\haiho\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (0.0.2)\n",
      "Requirement already satisfied: wget in c:\\users\\haiho\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (3.2)\n",
      "Collecting requests\n",
      "  Downloading requests-2.32.4-py3-none-any.whl (64 kB)\n",
      "     ---------------------------------------- 64.8/64.8 kB 1.2 MB/s eta 0:00:00\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\haiho\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from pandas) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in c:\\users\\haiho\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from pandas) (2025.2)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\haiho\\appdata\\roaming\\python\\python310\\site-packages (from pandas) (2.8.2)\n",
      "Requirement already satisfied: beautifulsoup4 in c:\\users\\haiho\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from bs4) (4.13.4)\n",
      "Collecting certifi>=2017.4.17\n",
      "  Downloading certifi-2025.7.14-py3-none-any.whl (162 kB)\n",
      "     -------------------------------------- 162.7/162.7 kB 3.2 MB/s eta 0:00:00\n",
      "Collecting urllib3<3,>=1.21.1\n",
      "  Downloading urllib3-2.5.0-py3-none-any.whl (129 kB)\n",
      "     -------------------------------------- 129.8/129.8 kB 7.5 MB/s eta 0:00:00\n",
      "Collecting charset_normalizer<4,>=2\n",
      "  Downloading charset_normalizer-3.4.2-cp310-cp310-win_amd64.whl (105 kB)\n",
      "     -------------------------------------- 105.8/105.8 kB 6.4 MB/s eta 0:00:00\n",
      "Collecting idna<4,>=2.5\n",
      "  Downloading idna-3.10-py3-none-any.whl (70 kB)\n",
      "     ---------------------------------------- 70.4/70.4 kB 3.8 MB/s eta 0:00:00\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\haiho\\appdata\\roaming\\python\\python310\\site-packages (from python-dateutil>=2.8.2->pandas) (1.16.0)\n",
      "Requirement already satisfied: typing-extensions>=4.0.0 in c:\\users\\haiho\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from beautifulsoup4->bs4) (4.14.1)\n",
      "Requirement already satisfied: soupsieve>1.2 in c:\\users\\haiho\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from beautifulsoup4->bs4) (2.7)\n",
      "Installing collected packages: urllib3, idna, charset_normalizer, certifi, requests\n",
      "Successfully installed certifi-2025.7.14 charset_normalizer-3.4.2 idna-3.10 requests-2.32.4 urllib3-2.5.0\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  WARNING: The script normalizer.exe is installed in 'c:\\Users\\haiho\\AppData\\Local\\Programs\\Python\\Python310\\Scripts' which is not on PATH.\n",
      "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\n",
      "\n",
      "[notice] A new release of pip available: 22.2.2 -> 25.1.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "%pip install numpy pandas bs4 wget requests"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Download the required exchange rate file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'List_of_largest_banks'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Download the CSV data first into a local `exchange_rate.csv` file\n",
    "import wget\n",
    "wget.download(\"https://web.archive.org/web/20230908091635/https://en.wikipedia.org/wiki/List_of_largest_banks\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Importing the required libraries:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import sqlite3\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup\n",
    "from datetime import datetime\n",
    "\n",
    "# Suppress generated warnings\n",
    "def warn(*args, **kwargs):\n",
    "    pass\n",
    "import warnings\n",
    "warnings.warn = warn\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tasks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 1: Logging function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This function accepts the message to be logged and enters it to the log file on a new line.\n",
    "\n",
    "Log entries will be in the format: `<timestamp> : <message>`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def log_progress(message):\n",
    "    ''' This function logs the mentioned message of a given stage of the\n",
    "    code execution to a log file. Function returns nothing'''\n",
    "\n",
    "    time_stamp = datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "    log_entry = f\"{time_stamp} : {message}\\n\"\n",
    "    with open(\"code_log.txt\", \"a\") as log_file:\n",
    "        log_file.write(log_entry)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 2: Extraction of data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Inspect the URL and identify the position and pattern of the tabular information in the HTML code under the heading `By market capitalization`. In the given webpage, our table is at the first position, or index 0 and we require the entries under *\"Bank name\"* and *\"Market cap (US$ billion)\"*.\n",
    "\n",
    "    <img src=\"List_of_largest_banks.png\">\n",
    "\n",
    "    Notice that all rows under this table are mentioned as `tr` objects under the table. Clicking one of them reveals that the data in each row is further saved as a `td` object. \n",
    "    \n",
    "    It is also important to note that entries under \"Bank name\" have two hyperlinks associated with it, one for the Country and the other for Bank name, as seen in the image above.\n",
    "\n",
    "2. Write the function `extract()` to retrieve the information of the table to a Pandas DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract(url, table_attribs):\n",
    "    ''' This function aims to extract the required\n",
    "    information from the website and save it to a data frame. The\n",
    "    function returns the data frame for further processing. '''\n",
    "\n",
    "    page = requests.get(url).text\n",
    "    soup = BeautifulSoup(page, \"html.parser\")\n",
    "\n",
    "    df = pd.DataFrame(columns=table_attribs)\n",
    "\n",
    "    tables = soup.find_all(\"tbody\")\n",
    "    rows = tables[0].find_all(\"tr\")\n",
    "\n",
    "    for row in rows:\n",
    "        col = row.find_all(\"td\")\n",
    "        if len(col) != 0:\n",
    "            data_dict = {\"Name\": col[1].find_all(\"a\")[1][\"title\"],\n",
    "                         \"MC_USD_Billion\": float(col[2].contents[0][:-1])}\n",
    "            df1 = pd.DataFrame(data_dict, index=[0])\n",
    "            df = pd.concat([df, df1], ignore_index=True)\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The code functions as follows:\n",
    "\n",
    "1. Extract the web page as text.\n",
    "1. Parse the text into an HTML object.\n",
    "1. Create an empty Pandas DataFrame named `df` with columns argument set as `table_attribs`.\n",
    "1. Extract all `tbody` attributes of the HTML object and then extract all the rows of the index 0 table using the `tr` attribute.\n",
    "1. Iterate over the contents of the variable `rows`.\n",
    "1. Extract all the `td` data objects in the row and save them to `col`.\n",
    "1. Check if the length of `col` is 0, that is, if there is no data in the current row. This is important since, many times there are merged rows that are not apparent in the web page appearance.\n",
    "1. Create a dictionary `data_dict` with the keys same as the columns of the dataframe created for recording the output earlier and corresponding values from the second and third headers of data.\n",
    "    - Extract the `title` attribute of the second hyperlink from the `Bank name` column.\n",
    "    - Remove the last character (`\\n`) from the `Market cap (US$ billion)` column contents using negative index slicing, then typecast the value to `float` format.\n",
    "1. Convert the dictionary to a dataframe and concatenate it with the existing one. This way, the data keeps getting appended to the dataframe with every iteration of the loop.\n",
    "1. Return the dataframe."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 3: Transformation of data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Transform function needs to perform the following tasks:\n",
    "\n",
    "1. Read the exchange rate CSV file and convert the contents to a dictionary so that the contents of the first columns are the keys to the dictionary and the contents of the second column are the corresponding values.\n",
    "\n",
    "1. Add 3 different columns to the dataframe: `MC_GBP_Billion`, `MC_EUR_Billion` and `MC_INR_Billion`, each containing the content of MC_USD_Billion scaled by the corresponding exchange rate factor. Round the resulting data to 2 decimal places."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform(df, csv_path):\n",
    "    ''' This function accesses the CSV file for exchange rate\n",
    "    information, and adds three columns to the data frame, each\n",
    "    containing the transformed version of Market Cap column to\n",
    "    respective currencies'''\n",
    "\n",
    "    # Read exchange rate CSV file\n",
    "    exchange_rate = pd.read_csv(csv_path)\n",
    "\n",
    "    # Convert to a dictionary with \"Currency\" as keys and \"Rate\" as values\n",
    "    exchange_rate = exchange_rate.set_index(\"Currency\").to_dict()[\"Rate\"]\n",
    "\n",
    "    # Add MC_GBP_Billion, MC_EUR_Billion, and MC_INR_Billion\n",
    "    # columns to dataframe. Round off to two decimals\n",
    "    df[\"MC_GBP_Billion\"] = [np.round(x * exchange_rate[\"GBP\"], 2) for x in df[\"MC_USD_Billion\"]]\n",
    "    df[\"MC_EUR_Billion\"] = [np.round(x * exchange_rate[\"EUR\"], 2) for x in df[\"MC_USD_Billion\"]]\n",
    "    df[\"MC_INR_Billion\"] = [np.round(x * exchange_rate[\"INR\"], 2) for x in df[\"MC_USD_Billion\"]]\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 4: Loading to CSV"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the transformed dataframe to an output CSV file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_to_csv(df, output_path):\n",
    "    ''' This function saves the final data frame as a CSV file in\n",
    "    the provided path. Function returns nothing.'''\n",
    "\n",
    "    df.to_csv(output_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 5: Loading to Database"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the transformed dataframe to a SQL database server as a table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_to_db(df, sql_connection, table_name):\n",
    "    ''' This function saves the final data frame to a database\n",
    "    table with the provided name. Function returns nothing.'''\n",
    "\n",
    "    df.to_sql(table_name, sql_connection, if_exists='replace', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 6: Function to Run queries on Database"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run queries on the database table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_query(query_statement, sql_connection):\n",
    "    ''' This function runs the query on the database table and\n",
    "    prints the output on the terminal. Function returns nothing. '''\n",
    "\n",
    "    print(query_statement)\n",
    "    query_output = pd.read_sql(query_statement, sql_connection)\n",
    "    print(query_output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Running ETL Process"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Declaring known values:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = \"https://web.archive.org/web/20230908091635/https://en.wikipedia.org/wiki/List_of_largest_banks\"\n",
    "csv_path = \"./exchange_rate.csv\"\n",
    "table_attribs = [\"Name\", \"MC_USD_Billion\"]\n",
    "output_path = \"./Largest_banks_data.csv\"\n",
    "db_name = \"Banks.db\"\n",
    "table_name = \"Largest_banks\"\n",
    "log_file = \"./code_log.txt\"\n",
    "\n",
    "log_progress(\"Preliminaries complete. Initiating ETL process\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Call **extract()** function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                      Name  MC_USD_Billion\n",
      "0                           JPMorgan Chase          432.92\n",
      "1                          Bank of America          231.52\n",
      "2  Industrial and Commercial Bank of China          194.56\n",
      "3               Agricultural Bank of China          160.68\n",
      "4                                HDFC Bank          157.91\n",
      "5                              Wells Fargo          155.87\n",
      "6                                     HSBC          148.90\n",
      "7                           Morgan Stanley          140.83\n",
      "8                  China Construction Bank          139.82\n",
      "9                            Bank of China          136.81\n"
     ]
    }
   ],
   "source": [
    "df = extract(url, table_attribs)\n",
    "print(df)\n",
    "\n",
    "log_progress(\"Data extraction complete. Initiating Transformation process\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Call **transform()** function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                      Name  MC_USD_Billion  MC_GBP_Billion  \\\n",
      "0                           JPMorgan Chase          432.92          346.34   \n",
      "1                          Bank of America          231.52          185.22   \n",
      "2  Industrial and Commercial Bank of China          194.56          155.65   \n",
      "3               Agricultural Bank of China          160.68          128.54   \n",
      "4                                HDFC Bank          157.91          126.33   \n",
      "5                              Wells Fargo          155.87          124.70   \n",
      "6                                     HSBC          148.90          119.12   \n",
      "7                           Morgan Stanley          140.83          112.66   \n",
      "8                  China Construction Bank          139.82          111.86   \n",
      "9                            Bank of China          136.81          109.45   \n",
      "\n",
      "   MC_EUR_Billion  MC_INR_Billion  \n",
      "0          402.62        35910.71  \n",
      "1          215.31        19204.58  \n",
      "2          180.94        16138.75  \n",
      "3          149.43        13328.41  \n",
      "4          146.86        13098.63  \n",
      "5          144.96        12929.42  \n",
      "6          138.48        12351.26  \n",
      "7          130.97        11681.85  \n",
      "8          130.03        11598.07  \n",
      "9          127.23        11348.39  \n"
     ]
    }
   ],
   "source": [
    "df = transform(df, csv_path)\n",
    "print(df)\n",
    "\n",
    "log_progress(\"Data transformation complete. Initiating Loading process\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Call **load_to_csv()** function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "load_to_csv(df, output_path)\n",
    "\n",
    "log_progress(\"Data saved to CSV file\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Initiate **SQLite3** connection:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "sql_connection = sqlite3.connect(db_name)\n",
    "\n",
    "log_progress(\"SQL Connection initiated\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Call **load_to_db()** function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "load_to_db(df, sql_connection, table_name)\n",
    "\n",
    "log_progress(\"Data loaded to Database as a table, Executing queries\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Call **run_query()** function: "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Print the contents of the entire table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SELECT * from Largest_banks\n",
      "                                      Name  MC_USD_Billion  MC_GBP_Billion  \\\n",
      "0                           JPMorgan Chase          432.92          346.34   \n",
      "1                          Bank of America          231.52          185.22   \n",
      "2  Industrial and Commercial Bank of China          194.56          155.65   \n",
      "3               Agricultural Bank of China          160.68          128.54   \n",
      "4                                HDFC Bank          157.91          126.33   \n",
      "5                              Wells Fargo          155.87          124.70   \n",
      "6                                     HSBC          148.90          119.12   \n",
      "7                           Morgan Stanley          140.83          112.66   \n",
      "8                  China Construction Bank          139.82          111.86   \n",
      "9                            Bank of China          136.81          109.45   \n",
      "\n",
      "   MC_EUR_Billion  MC_INR_Billion  \n",
      "0          402.62        35910.71  \n",
      "1          215.31        19204.58  \n",
      "2          180.94        16138.75  \n",
      "3          149.43        13328.41  \n",
      "4          146.86        13098.63  \n",
      "5          144.96        12929.42  \n",
      "6          138.48        12351.26  \n",
      "7          130.97        11681.85  \n",
      "8          130.03        11598.07  \n",
      "9          127.23        11348.39  \n"
     ]
    }
   ],
   "source": [
    "query_statement = f\"SELECT * from {table_name}\"\n",
    "run_query(query_statement, sql_connection)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Print the average market capitalization of all the banks in Billion GBP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SELECT AVG(MC_GBP_Billion) FROM Largest_banks\n",
      "   AVG(MC_GBP_Billion)\n",
      "0              151.987\n"
     ]
    }
   ],
   "source": [
    "query_statement = f\"SELECT AVG(MC_GBP_Billion) FROM {table_name}\"\n",
    "run_query(query_statement, sql_connection)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Print only the names of the top 5 banks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SELECT Name from Largest_banks LIMIT 5\n",
      "                                      Name\n",
      "0                           JPMorgan Chase\n",
      "1                          Bank of America\n",
      "2  Industrial and Commercial Bank of China\n",
      "3               Agricultural Bank of China\n",
      "4                                HDFC Bank\n"
     ]
    }
   ],
   "source": [
    "query_statement = f\"SELECT Name from {table_name} LIMIT 5\"\n",
    "run_query(query_statement, sql_connection)\n",
    "\n",
    "log_progress(\"Process Complete\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Close **SQLite3** connection:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "sql_connection.close()\n",
    "\n",
    "log_progress(\"Server Connection closed\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Task 7: Verify log entries\n",
    "\n",
    "Upon successful completion of execution, you should see all the relevant entries made in the log file in relation to the stages of code execution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-07-15 11:08:18 : Preliminaries complete. Initiating ETL process\n",
      "2025-07-15 11:08:18 : Data extraction complete. Initiating Transformation process\n",
      "2025-07-15 11:08:18 : Data transformation complete. Initiating Transformation process\n",
      "2025-07-15 11:08:18 : Data saved to CSV file\n",
      "2025-07-15 11:08:18 : SQL Connection initiated\n",
      "2025-07-15 11:08:18 : Data loaded to Database as a table, Executing queries\n",
      "2025-07-15 11:08:18 : Process Complete\n",
      "2025-07-15 11:08:18 : Server Connection closed\n",
      "2025-07-17 00:09:06 : Preliminaries complete. Initiating ETL process\n",
      "2025-07-17 00:09:26 : Data extraction complete. Initiating Transformation process\n",
      "2025-07-17 00:09:42 : Data transformation complete. Initiating Loading process\n",
      "2025-07-17 00:09:54 : Data saved to CSV file\n",
      "2025-07-17 00:10:09 : SQL Connection initiated\n",
      "2025-07-17 00:10:21 : Data loaded to Database as a table, Executing queries\n",
      "2025-07-17 00:10:56 : Process Complete\n",
      "2025-07-17 00:11:08 : Server Connection closed\n",
      "\n"
     ]
    }
   ],
   "source": [
    "with open(log_file, \"r\") as log:\n",
    "    LogContent = log.read()\n",
    "    print(LogContent)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Change Log\n",
    "\n",
    "\n",
    "|  Date (YYYY-MM-DD) |  Version | Changed By  |  Change Description |\n",
    "|---|---|---|---|\n",
    "| 2025-07-16  | 0.1  | Hoang H. Nguyen | Initial version |"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
